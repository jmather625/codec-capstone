{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6595e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3f3e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatin/miniconda3/envs/DCVC/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.models.DCVC_net import DCVC_net\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import tqdm\n",
    "from torchnet import meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ba9f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAMBDA = 2048\n",
    "BATCH_SIZE = 4\n",
    "DATA_DIR = pathlib.Path('/data2/jatin/vimeo_septuplet/sequences')\n",
    "DEVICE = torch.device('cuda')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046ab39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f'b_frame_lamba={LAMBDA}-dcvc-train-procedure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61c7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcodec-crew\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jatin/codec/codec-capstone/DCVC_Bframe_mod/wandb/run-20220508_194533-2wjlyy12</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/codec-crew/DCVC-B-frames/runs/2wjlyy12\" target=\"_blank\">b_frame_lamba=2048-dcvc-train-procedure</a></strong> to <a href=\"https://wandb.ai/codec-crew/DCVC-B-frames\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/codec-crew/DCVC-B-frames/runs/2wjlyy12?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f60c4935610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"DCVC-B-frames\", \n",
    "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=exp_name, \n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"architecture\": \"DCVC\",\n",
    "    \"dataset\": \"UVG\",\n",
    "    \"epochs\": 20,\n",
    "    },\n",
    "    resume=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3b397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_net = DCVC_net(up_strategy='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7626553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the good weights\n",
    "video_net.opticFlow = torch.load('../DCVC-Old/optflow.pth')\n",
    "# these keys do not match, so we remove it from the network and add it back later\n",
    "# temporalPriorEncoder = video_net.temporalPriorEncoder\n",
    "# del video_net.temporalPriorEncoder\n",
    "\n",
    "# chpt = torch.load('dcvc-b-frame-with-bitrate-lambda-2048-tryfix-bitrate/dcvc_epoch=2_int_allquantize.pt')\n",
    "# chpt = torch.load('b_frame_lamba=2048-old-continue/dcvc_epoch=5_int.pt')\n",
    "# video_net.load_state_dict(chpt['model'], strict=True)\n",
    "\n",
    "video_net = video_net.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(video_net.parameters(), lr=wandb.config.learning_rate)\n",
    "# optimizer.load_state_dict(chpt['optimizer'])\n",
    "\n",
    "# video_net.opticFlow.requires_grad_ = False\n",
    "# video_net.mvEncoder.requires_grad_ = False\n",
    "# video_net.mvDecoder_part1.requires_grad_ = False\n",
    "# video_net.mvDecoder_part2.requires_grad_ = False\n",
    "# video_net.feature_extract.requires_grad_ = False\n",
    "# video_net.context_refine.requires_grad_ = False\n",
    "# video_net.contextualDecoder_part1.requires_grad_ = False\n",
    "\n",
    "# del chpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525a3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, crop_size=256, make_b_cut=True, deterministic=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.crop_size = crop_size\n",
    "        self.make_b_cut = make_b_cut\n",
    "        self.deterministic = deterministic\n",
    "        self.all_paths = []\n",
    "        for seq in os.listdir(self.data_dir):\n",
    "            subseq = os.listdir(self.data_dir / seq)\n",
    "            for s in subseq:\n",
    "                self.all_paths.append(self.data_dir / seq / s)\n",
    "        assert len(self.all_paths) == 91701\n",
    "        \n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            transforms.RandomCrop(crop_size)\n",
    "        )\n",
    "       \n",
    "    def __getitem__(self, i):\n",
    "        path = self.all_paths[i]\n",
    "        imgs = []\n",
    "        if self.make_b_cut:\n",
    "            # load two reference frames and the B-frame in the middle\n",
    "            #TODO: implement making this deterministic\n",
    "            interval = np.random.randint(1, 4) # can be 1, 2, or 3\n",
    "            ref1 = plt.imread(path / f'im{1}.png')\n",
    "            ref2 = plt.imread(path / f'im{1 + interval*2}.png')\n",
    "            # this is the B-frame, in the middle\n",
    "            im = plt.imread(path / f'im{1 + interval}.png')\n",
    "            imgs = [ref1, ref2, im]\n",
    "        else:\n",
    "            # load full sequence\n",
    "            for i in range(1, 8):\n",
    "                # should be between [0, 1]\n",
    "                img = plt.imread(path / f'im{i}.png')\n",
    "        \n",
    "        # plt.imread should make inputs in [0, 1] for us\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "        # bring RGB channels in front\n",
    "        imgs = imgs.transpose(0, 3, 1, 2)\n",
    "        return self.transforms(torch.FloatTensor(imgs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "ds = VideoDataset(DATA_DIR)\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=6,\n",
    "    prefetch_factor=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62637e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f67b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 17969762 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Return number of parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(video_net)} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cddd520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_to_psnr(mse):\n",
    "    return -10.0*np.log10(mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3dd62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, epoch, dl, optimizer, train_type):\n",
    "    mse_criterion = torch.nn.MSELoss()\n",
    "    model.train()\n",
    "    \n",
    "    if train_type == 'memc':\n",
    "        mse1_meter = meter.AverageValueMeter()\n",
    "        mse2_meter = meter.AverageValueMeter()\n",
    "    elif train_type == 'memc_bpp':\n",
    "        mse1_meter = meter.AverageValueMeter()\n",
    "        mse2_meter = meter.AverageValueMeter()\n",
    "        bpp_meter = meter.AverageValueMeter()\n",
    "    elif train_type == 'recon':\n",
    "        mse_meter = meter.AverageValueMeter()\n",
    "    else:\n",
    "        mse_meter = meter.AverageValueMeter()\n",
    "        bpp_meter = meter.AverageValueMeter()\n",
    "        bpp_mv_y_meter = meter.AverageValueMeter()\n",
    "        bpp_mv_z_meter = meter.AverageValueMeter()\n",
    "        bpp_y_meter = meter.AverageValueMeter()\n",
    "        bpp_z_meter = meter.AverageValueMeter()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pbar = tqdm.tqdm(dl)\n",
    "    for i, x in enumerate(pbar):\n",
    "        x = x.to(DEVICE)\n",
    "        ref1 = x[:,0]\n",
    "        ref2 = x[:,1]\n",
    "        im = x[:,2]\n",
    "        preds_dict = model(ref1, ref2, im, compress_type='train_compress', train_type=train_type)\n",
    "        if train_type == 'memc':\n",
    "            preds_dict = video_net(ref1, ref2, im, compress_type='train_compress', train_type='memc')\n",
    "            mse1 = mse_criterion(preds_dict['pred1'], im)\n",
    "            mse2 = mse_criterion(preds_dict['pred2'], im)\n",
    "            mse = (mse1 + mse2)/2\n",
    "            mse.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse1_meter.add(mse1.item())\n",
    "            mse2_meter.add(mse2.item())\n",
    "            wandb_export = {\n",
    "                'train/psnr1': mse_to_psnr(mse1_meter.value()[0]),\n",
    "                'train/psnr2': mse_to_psnr(mse2_meter.value()[0]),\n",
    "            }\n",
    "        elif train_type == 'memc_bpp':\n",
    "            preds_dict = video_net(ref1, ref2, im, compress_type='train_compress', train_type='memc_bpp')\n",
    "            mse1 = mse_criterion(preds_dict['pred1'], im)\n",
    "            mse2 = mse_criterion(preds_dict['pred2'], im)\n",
    "            mse = (mse1 + mse2)/2\n",
    "            bpp = preds_dict['mv_z_bpp'] + preds_dict['mv_y_bpp']\n",
    "            loss = mse * LAMBDA + bpp\n",
    "            loss.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse1_meter.add(mse1.item())\n",
    "            mse2_meter.add(mse2.item())\n",
    "            bpp_meter.add(bpp.item())\n",
    "            wandb_export = {\n",
    "                'train/psnr1': mse_to_psnr(mse1_meter.value()[0]),\n",
    "                'train/psnr2': mse_to_psnr(mse2_meter.value()[0]),\n",
    "                'train/bpp': bpp_meter.value()[0]\n",
    "            }\n",
    "        elif train_type == 'recon':\n",
    "            preds_dict = video_net(ref1, ref2, im, compress_type='train_compress', train_type='recon')\n",
    "            mse = mse_criterion(preds_dict['recon_image'], im)\n",
    "            mse.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse_meter.add(mse.item())\n",
    "            wandb_export = {\n",
    "                'train/psnr_recon': mse_to_psnr(mse_meter.value()[0]),\n",
    "            }\n",
    "        else:\n",
    "            preds_dict = video_net(ref1, ref2, im, compress_type='train_compress', train_type='full')\n",
    "            mse = mse_criterion(preds_dict['recon_image'], im)\n",
    "            loss = mse * LAMBDA + preds_dict['bpp']\n",
    "            loss.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse_meter.add(mse.item())\n",
    "            bpp_meter.add(preds_dict['bpp'].item())\n",
    "            bpp_mv_y_meter.add(preds_dict['bpp_mv_y'].item())\n",
    "            bpp_mv_z_meter.add(preds_dict['bpp_mv_z'].item())\n",
    "            bpp_y_meter.add(preds_dict['bpp_y'].item())\n",
    "            bpp_z_meter.add(preds_dict['bpp_z'].item())\n",
    "            \n",
    "            wandb_export = {\n",
    "                'train/psnr_recon': mse_to_psnr(mse_meter.value()[0]),\n",
    "                'train/bpp': bpp_meter.value()[0],\n",
    "                'train/bpp_mv_y': bpp_mv_y_meter.value()[0],\n",
    "                'train/bpp_mv_z': bpp_mv_z_meter.value()[0],\n",
    "                'train/bpp_y': bpp_y_meter.value()[0],\n",
    "                'train/bpp_z': bpp_z_meter.value()[0],\n",
    "            }\n",
    "            \n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            wandb_export['train/epoch'] = epoch\n",
    "            wandb_export['train/train_type'] = train_type\n",
    "            wandb.log(wandb_export)\n",
    "        # save every \n",
    "        if i % 4000 == 3999:\n",
    "            print('Saving model')\n",
    "            torch.save(\n",
    "                {'model': model.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "                SAVE_FOLDER / f\"dcvc_epoch={epoch}_batch_{i}.pt\",\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c48c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FOLDER = pathlib.Path(exp_name)\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0900f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layer(l):\n",
    "    for p in l.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_layer(l):\n",
    "    for p in l.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab3bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3118/22926 [12:01<1:14:47,  4.41it/s]"
     ]
    }
   ],
   "source": [
    "freeze_layer(video_net.opticFlow)\n",
    "\n",
    "train_type = 'memc'\n",
    "train_epoch(video_net, 1, dl, optimizer, train_type=train_type)\n",
    "torch.save(\n",
    "    {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "    SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    ")\n",
    "\n",
    "train_type = 'memc_bpp'\n",
    "for i in range(1, 4):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "\n",
    "# freeze mv layers\n",
    "mv_layers = [\n",
    "    video_net.bitEstimator_z_mv,\n",
    "    video_net.mvpriorEncoder,\n",
    "    video_net.mvpriorDecoder,\n",
    "    video_net.mvDecoder_part1,\n",
    "    video_net.mvDecoder_part2,\n",
    "    video_net.auto_regressive_mv,\n",
    "    video_net.entropy_parameters_mv,\n",
    "]\n",
    "print('Freezing MV layers')\n",
    "for l in mv_layers:\n",
    "    freeze_layer(l)\n",
    "\n",
    "train_type = 'recon'\n",
    "for i in range(4, 10):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "\n",
    "train_type = 'full'\n",
    "for i in range(10, 13):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "    \n",
    "# now unfreeze\n",
    "print('Unfreezing MV layers and optical flow for end-to-end training')\n",
    "for l in [video_net.opticFlow] + mv_layers:\n",
    "    unfreeze_layer(l)\n",
    "    \n",
    "for i in range(13, 19):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cf4660b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/avg_bitrate</td><td>█▆▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/avg_loss</td><td>█▇▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/avg_mse_loss</td><td>██▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/avg_psnr</td><td>▁▁▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▇▇▇▇███▇████████</td></tr><tr><td>train/batch_loss</td><td>█▆▅▄▄▃▄▂▃▃▃▂▃▃▄▂▁▂▃▃▂▂▃▄▂▁▅▂▂▁▂▂▁▂▂▂▁▁▁▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/avg_bitrate</td><td>0.02835</td></tr><tr><td>train/avg_loss</td><td>0.12763</td></tr><tr><td>train/avg_mse_loss</td><td>5e-05</td></tr><tr><td>train/avg_psnr</td><td>43.14486</td></tr><tr><td>train/batch_loss</td><td>0.07535</td></tr><tr><td>train/epoch</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">experiment_b_frame_default_with_bitrate_lambda=2048</strong>: <a href=\"https://wandb.ai/codec-crew/DCVC-B-frames/runs/1o59wjpv\" target=\"_blank\">https://wandb.ai/codec-crew/DCVC-B-frames/runs/1o59wjpv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220428_151548-1o59wjpv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ebe34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcvc",
   "language": "python",
   "name": "dcvc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
