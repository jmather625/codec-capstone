{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6595e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3f3e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatin/miniconda3/envs/DCVC/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.models.DCVC_net import DCVC_net\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import tqdm\n",
    "from torchnet import meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ba9f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAMBDA = 2048\n",
    "BATCH_SIZE = 4\n",
    "DATA_DIR = pathlib.Path('/data2/jatin/vimeo_septuplet/sequences')\n",
    "DEVICE = torch.device('cuda')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046ab39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f'b_frame_lamba={LAMBDA}-dcvc-train-procedure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61c7472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcodec-crew\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jatin/codec/codec-capstone/DCVC_Bframe_mod/wandb/run-20220510_064714-2wjlyy12</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/codec-crew/DCVC-B-frames/runs/2wjlyy12\" target=\"_blank\">b_frame_lamba=2048-dcvc-train-procedure</a></strong> to <a href=\"https://wandb.ai/codec-crew/DCVC-B-frames\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/codec-crew/DCVC-B-frames/runs/2wjlyy12?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f5c2c3d1550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"DCVC-B-frames\", \n",
    "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=exp_name, \n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"architecture\": \"DCVC\",\n",
    "    \"dataset\": \"UVG\",\n",
    "    \"epochs\": 20,\n",
    "    },\n",
    "    resume=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3b397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_net = DCVC_net(up_strategy='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7626553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the good weights\n",
    "# video_net.opticFlow = torch.load('../DCVC-Old/optflow.pth')\n",
    "# these keys do not match, so we remove it from the network and add it back later\n",
    "# temporalPriorEncoder = video_net.temporalPriorEncoder\n",
    "# del video_net.temporalPriorEncoder\n",
    "\n",
    "# chpt = torch.load('dcvc-b-frame-with-bitrate-lambda-2048-tryfix-bitrate/dcvc_epoch=2_int_allquantize.pt')\n",
    "# chpt = torch.load('b_frame_lamba=2048-old-continue/dcvc_epoch=5_int.pt')\n",
    "chpt = torch.load(exp_name + '/dcvc_memc_bpp_epoch=3.pt')\n",
    "# video_net.load_state_dict(chpt['model'], strict=True)\n",
    "\n",
    "video_net = video_net.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(video_net.parameters(), lr=wandb.config.learning_rate)\n",
    "optimizer.load_state_dict(chpt['optimizer'])\n",
    "\n",
    "# video_net.opticFlow.requires_grad_ = False\n",
    "# video_net.mvEncoder.requires_grad_ = False\n",
    "# video_net.mvDecoder_part1.requires_grad_ = False\n",
    "# video_net.mvDecoder_part2.requires_grad_ = False\n",
    "# video_net.feature_extract.requires_grad_ = False\n",
    "# video_net.context_refine.requires_grad_ = False\n",
    "# video_net.contextualDecoder_part1.requires_grad_ = False\n",
    "\n",
    "del chpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525a3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, crop_size=256, make_b_cut=True, deterministic=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.crop_size = crop_size\n",
    "        self.make_b_cut = make_b_cut\n",
    "        self.deterministic = deterministic\n",
    "        self.all_paths = []\n",
    "        for seq in os.listdir(self.data_dir):\n",
    "            subseq = os.listdir(self.data_dir / seq)\n",
    "            for s in subseq:\n",
    "                self.all_paths.append(self.data_dir / seq / s)\n",
    "        assert len(self.all_paths) == 91701\n",
    "        \n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            transforms.RandomCrop(crop_size)\n",
    "        )\n",
    "       \n",
    "    def __getitem__(self, i):\n",
    "        path = self.all_paths[i]\n",
    "        imgs = []\n",
    "        if self.make_b_cut:\n",
    "            # load two reference frames and the B-frame in the middle\n",
    "            #TODO: implement making this deterministic\n",
    "            interval = np.random.randint(1, 4) # can be 1, 2, or 3\n",
    "            ref1 = plt.imread(path / f'im{1}.png')\n",
    "            ref2 = plt.imread(path / f'im{1 + interval*2}.png')\n",
    "            # this is the B-frame, in the middle\n",
    "            im = plt.imread(path / f'im{1 + interval}.png')\n",
    "            imgs = [ref1, ref2, im]\n",
    "        else:\n",
    "            # load full sequence\n",
    "            for i in range(1, 8):\n",
    "                # should be between [0, 1]\n",
    "                img = plt.imread(path / f'im{i}.png')\n",
    "        \n",
    "        # plt.imread should make inputs in [0, 1] for us\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "        # bring RGB channels in front\n",
    "        imgs = imgs.transpose(0, 3, 1, 2)\n",
    "        return self.transforms(torch.FloatTensor(imgs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "ds = VideoDataset(DATA_DIR)\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=6,\n",
    "    prefetch_factor=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62637e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f67b8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 17969762 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Return number of parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(video_net)} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bcfa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_to_psnr(mse):\n",
    "    return -10.0*np.log10(mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3dd62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, epoch, dl, optimizer, train_type):\n",
    "    mse_criterion = torch.nn.MSELoss()\n",
    "    model.train()\n",
    "    \n",
    "    if train_type == 'memc':\n",
    "        mse1_meter = meter.AverageValueMeter()\n",
    "        mse2_meter = meter.AverageValueMeter()\n",
    "    elif train_type == 'memc_bpp':\n",
    "        mse1_meter = meter.AverageValueMeter()\n",
    "        mse2_meter = meter.AverageValueMeter()\n",
    "        bpp_meter = meter.AverageValueMeter()\n",
    "    elif train_type == 'recon':\n",
    "        mse_meter = meter.AverageValueMeter()\n",
    "    else:\n",
    "        mse_meter = meter.AverageValueMeter()\n",
    "        bpp_meter = meter.AverageValueMeter()\n",
    "        bpp_mv_y_meter = meter.AverageValueMeter()\n",
    "        bpp_mv_z_meter = meter.AverageValueMeter()\n",
    "        bpp_y_meter = meter.AverageValueMeter()\n",
    "        bpp_z_meter = meter.AverageValueMeter()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pbar = tqdm.tqdm(dl)\n",
    "    for i, x in enumerate(pbar):\n",
    "        x = x.to(DEVICE)\n",
    "        ref1 = x[:,0]\n",
    "        ref2 = x[:,1]\n",
    "        im = x[:,2]\n",
    "        preds_dict = model(ref1, ref2, im, compress_type='train_compress', train_type=train_type)\n",
    "        if train_type == 'memc':\n",
    "            mse1 = mse_criterion(preds_dict['pred1'], im)\n",
    "            mse2 = mse_criterion(preds_dict['pred2'], im)\n",
    "            mse = (mse1 + mse2)/2\n",
    "            mse.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse1_meter.add(mse1.item())\n",
    "            mse2_meter.add(mse2.item())\n",
    "            wandb_export = {\n",
    "                'train/psnr1': mse_to_psnr(mse1_meter.value()[0]),\n",
    "                'train/psnr2': mse_to_psnr(mse2_meter.value()[0]),\n",
    "            }\n",
    "        elif train_type == 'memc_bpp':\n",
    "            mse1 = mse_criterion(preds_dict['pred1'], im)\n",
    "            mse2 = mse_criterion(preds_dict['pred2'], im)\n",
    "            mse = (mse1 + mse2)/2\n",
    "            bpp = preds_dict['mv_z_bpp'] + preds_dict['mv_y_bpp']\n",
    "            loss = mse * LAMBDA + bpp\n",
    "            loss.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse1_meter.add(mse1.item())\n",
    "            mse2_meter.add(mse2.item())\n",
    "            bpp_meter.add(bpp.item())\n",
    "            wandb_export = {\n",
    "                'train/psnr1': mse_to_psnr(mse1_meter.value()[0]),\n",
    "                'train/psnr2': mse_to_psnr(mse2_meter.value()[0]),\n",
    "                'train/bpp': bpp_meter.value()[0]\n",
    "            }\n",
    "        elif train_type == 'recon':\n",
    "            mse = mse_criterion(preds_dict['recon_image'], im)\n",
    "            mse.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse_meter.add(mse.item())\n",
    "            wandb_export = {\n",
    "                'train/psnr_recon': mse_to_psnr(mse_meter.value()[0]),\n",
    "            }\n",
    "        else:\n",
    "            mse = mse_criterion(preds_dict['recon_image'], im)\n",
    "            loss = mse * LAMBDA + preds_dict['bpp']\n",
    "            loss.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse_meter.add(mse.item())\n",
    "            bpp_meter.add(preds_dict['bpp'].item())\n",
    "            bpp_mv_y_meter.add(preds_dict['bpp_mv_y'].item())\n",
    "            bpp_mv_z_meter.add(preds_dict['bpp_mv_z'].item())\n",
    "            bpp_y_meter.add(preds_dict['bpp_y'].item())\n",
    "            bpp_z_meter.add(preds_dict['bpp_z'].item())\n",
    "            \n",
    "            wandb_export = {\n",
    "                'train/psnr_recon': mse_to_psnr(mse_meter.value()[0]),\n",
    "                'train/bpp': bpp_meter.value()[0],\n",
    "                'train/bpp_mv_y': bpp_mv_y_meter.value()[0],\n",
    "                'train/bpp_mv_z': bpp_mv_z_meter.value()[0],\n",
    "                'train/bpp_y': bpp_y_meter.value()[0],\n",
    "                'train/bpp_z': bpp_z_meter.value()[0],\n",
    "            }\n",
    "            \n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            wandb_export['train/epoch'] = epoch\n",
    "            wandb_export['train/train_type'] = train_type\n",
    "            wandb.log(wandb_export)\n",
    "        # save every \n",
    "        if i % 4000 == 3999:\n",
    "            print('Saving model')\n",
    "            torch.save(\n",
    "                {'model': model.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "                SAVE_FOLDER / f\"dcvc_epoch={epoch}_batch_{i}.pt\",\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c48c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FOLDER = pathlib.Path(exp_name)\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56bb2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layer(l):\n",
    "    for p in l.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_layer(l):\n",
    "    for p in l.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a16dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing MV layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2782/22926 [14:26<1:44:15,  3.22it/s]wandb: Network error (ReadTimeout), entering retry loop.\n",
      " 17%|█▋        | 3871/22926 [20:05<1:38:44,  3.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 87%|████████▋ | 19999/22926 [1:43:39<15:09,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 21057/22926 [1:49:08<09:40,  3.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 70%|██████▉   | 15937/22926 [1:22:36<36:18,  3.21it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 35%|███▍      | 7999/22926 [41:28<1:17:15,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 10169/22926 [52:43<1:06:11,  3.21it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 17%|█▋        | 3999/22926 [20:44<1:38:05,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4356/22926 [22:35<1:36:11,  3.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 70%|██████▉   | 15999/22926 [1:22:56<35:50,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 18425/22926 [1:35:31<23:20,  3.21it/s]wandb: Network error (ReadTimeout), entering retry loop.\n",
      " 87%|████████▋ | 19999/22926 [1:43:40<15:10,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22926/22926 [1:58:51<00:00,  3.21it/s]\n",
      " 17%|█▋        | 3999/22926 [20:44<1:38:03,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4215/22926 [21:51<1:36:54,  3.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 87%|████████▋ | 19999/22926 [1:43:42<15:10,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 20093/22926 [1:44:11<14:40,  3.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 95%|█████████▍| 21748/22926 [1:52:46<06:05,  3.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 64%|██████▍   | 14696/22926 [1:16:13<42:36,  3.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 70%|██████▉   | 15999/22926 [1:22:58<35:56,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 16346/22926 [1:24:46<34:06,  3.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 35%|███▍      | 7999/22926 [48:31<1:30:53,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 9258/22926 [56:10<1:22:51,  2.75it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 48%|████▊     | 10891/22926 [1:06:04<1:12:06,  2.78it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 17%|█▋        | 3999/22926 [24:16<1:54:33,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 4257/22926 [25:50<1:53:25,  2.74it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 87%|████████▋ | 19999/22926 [2:01:18<17:47,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 20503/22926 [2:04:22<14:42,  2.75it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 52%|█████▏    | 11999/22926 [1:12:45<1:05:47,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 14853/22926 [1:30:02<48:45,  2.76it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 35%|███▍      | 7999/22926 [1:05:56<2:02:50,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 8919/22926 [1:13:31<1:54:51,  2.03it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 13%|█▎        | 3088/22926 [25:25<2:42:44,  2.03it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 87%|████████▋ | 19999/22926 [2:44:40<24:02,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 20037/22926 [2:44:59<23:42,  2.03it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 52%|█████▏    | 11999/22926 [1:38:45<1:29:53,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 14115/22926 [1:56:11<1:12:23,  2.03it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 35%|███▍      | 7999/22926 [1:05:52<2:02:32,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 8506/22926 [1:10:03<1:58:34,  2.03it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 22926/22926 [3:08:49<00:00,  2.02it/s]\n",
      " 11%|█▏        | 2588/22926 [21:20<2:46:35,  2.03it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 86%|████████▌ | 19765/22926 [2:42:49<25:56,  2.03it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 52%|█████▏    | 11999/22926 [1:38:54<1:30:13,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 13755/22926 [1:53:23<1:15:47,  2.02it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freeze_layer(video_net.opticFlow)\n",
    "\n",
    "# train_type = 'memc'\n",
    "# train_epoch(video_net, 0, dl, optimizer, train_type=train_type)\n",
    "# torch.save(\n",
    "#     {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "#     SAVE_FOLDER / f\"dcvc_{train_type}_epoch=0.pt\",\n",
    "# )\n",
    "\n",
    "# train_type = 'memc_bpp'\n",
    "# for i in range(1, 4):\n",
    "#     train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "#     torch.save(\n",
    "#         {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "#         SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "#     )\n",
    "\n",
    "# freeze mv layers\n",
    "mv_layers = [\n",
    "    video_net.bitEstimator_z_mv,\n",
    "    video_net.mvpriorEncoder,\n",
    "    video_net.mvpriorDecoder,\n",
    "    video_net.mvDecoder_part1,\n",
    "    video_net.mvDecoder_part2,\n",
    "    video_net.auto_regressive_mv,\n",
    "    video_net.entropy_parameters_mv,\n",
    "]\n",
    "print('Freezing MV layers')\n",
    "for l in mv_layers:\n",
    "    freeze_layer(l)\n",
    "\n",
    "train_type = 'recon'\n",
    "for i in range(4, 10):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "\n",
    "train_type = 'full'\n",
    "for i in range(10, 13):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "    \n",
    "# now unfreeze\n",
    "print('Unfreezing MV layers and optical flow for end-to-end training')\n",
    "for l in [video_net.opticFlow] + mv_layers:\n",
    "    unfreeze_layer(l)\n",
    "    \n",
    "for i in range(13, 19):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf4660b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/bpp</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/bpp_mv_y</td><td>█▆▆▅▅▅▅▅▅▅▅▅▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/bpp_mv_z</td><td>█████████████▇▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/bpp_y</td><td>█▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/bpp_z</td><td>█▆▄▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/psnr_recon</td><td>▁▂▃▄▄▅▅▆▆▇▇▇▇███▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/bpp</td><td>0.21423</td></tr><tr><td>train/bpp_mv_y</td><td>0.00338</td></tr><tr><td>train/bpp_mv_z</td><td>2e-05</td></tr><tr><td>train/bpp_y</td><td>0.20896</td></tr><tr><td>train/bpp_z</td><td>0.00187</td></tr><tr><td>train/epoch</td><td>18</td></tr><tr><td>train/psnr1</td><td>29.75597</td></tr><tr><td>train/psnr2</td><td>29.90464</td></tr><tr><td>train/psnr_recon</td><td>38.28439</td></tr><tr><td>train/train_type</td><td>full</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">b_frame_lamba=2048-dcvc-train-procedure</strong>: <a href=\"https://wandb.ai/codec-crew/DCVC-B-frames/runs/2wjlyy12\" target=\"_blank\">https://wandb.ai/codec-crew/DCVC-B-frames/runs/2wjlyy12</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220510_064714-2wjlyy12/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ebe34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcvc",
   "language": "python",
   "name": "dcvc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
