{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jatin/miniconda3/envs/DCVC/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.models.DCVC_net import DCVC_net\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import tqdm\n",
    "import torchnet.meter as meter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAMBDA = 2048\n",
    "BATCH_SIZE = 4\n",
    "DATA_DIR = pathlib.Path('/data2/jatin/vimeo_septuplet/sequences')\n",
    "DEVICE = torch.device('cuda')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_net = DCVC_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = f'dcvc-reproduce_lamba={LAMBDA}-train-procedure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a4e32febc032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the good weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# video_net.opticFlow = torch.load('./optflow.pth')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvideo_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video_net' is not defined"
     ]
    }
   ],
   "source": [
    "# load the good weights\n",
    "# video_net.opticFlow = torch.load('./optflow.pth')\n",
    "video_net = video_net.to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(video_net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcodec-crew\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jatin/codec/codec-capstone/DCVC-Old/wandb/run-20220505_043558-xbqy86rn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/codec-crew/DCVC-B-frames/runs/xbqy86rn\" target=\"_blank\">reproduce-dcvc-lamb-2048-ft-hardquant</a></strong> to <a href=\"https://wandb.ai/codec-crew/DCVC-B-frames\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/codec-crew/DCVC-B-frames/runs/xbqy86rn?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd7350be670>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"DCVC-B-frames\", \n",
    "    # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "    name=exp_name, \n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"architecture\": \"DCVC\",\n",
    "    \"dataset\": \"Vimeo-90k\",\n",
    "    \"epochs\": 20,\n",
    "    \"resume\": True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, crop_size=256, make_p_cut=False, make_b_cut=True, deterministic=False):\n",
    "        if make_b_cut and make_p_cut:\n",
    "            raise ValueError('Can only choose one of B-frames or P-frames')\n",
    "        self.data_dir = data_dir\n",
    "        self.crop_size = crop_size\n",
    "        self.make_p_cut = make_p_cut\n",
    "        self.make_b_cut = make_b_cut\n",
    "        self.deterministic = deterministic\n",
    "        self.all_paths = []\n",
    "        for seq in os.listdir(self.data_dir):\n",
    "            subseq = os.listdir(self.data_dir / seq)\n",
    "            for s in subseq:\n",
    "                self.all_paths.append(self.data_dir / seq / s)\n",
    "        assert len(self.all_paths) == 91701\n",
    "        \n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            transforms.RandomCrop(crop_size)\n",
    "        )\n",
    "       \n",
    "    def __getitem__(self, i):\n",
    "        path = self.all_paths[i]\n",
    "        imgs = []\n",
    "        if self.make_b_cut:\n",
    "            # load two reference frames and the B-frame in the middle\n",
    "            #TODO: implement making this deterministic\n",
    "            interval = np.random.randint(1, 4) # can be 1, 2, or 3\n",
    "            ref1 = plt.imread(path / f'im{1}.png')\n",
    "            ref2 = plt.imread(path / f'im{1 + interval*2}.png')\n",
    "            # this is the B-frame, in the middle\n",
    "            im = plt.imread(path / f'im{1 + interval}.png')\n",
    "            imgs = [ref1, ref2, im]\n",
    "        elif self.make_p_cut:\n",
    "            ref = plt.imread(path / f'im1.png')\n",
    "            im = plt.imread(path / f'im2.png')\n",
    "            imgs = [ref, im]\n",
    "        else:\n",
    "            # load full sequence\n",
    "            for i in range(1, 8):\n",
    "                # should be between [0, 1]\n",
    "                img = plt.imread(path / f'im{i}.png')\n",
    "        \n",
    "        # plt.imread should make inputs in [0, 1] for us\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "        # bring RGB channels in front\n",
    "        imgs = imgs.transpose(0, 3, 1, 2)\n",
    "        return self.transforms(torch.FloatTensor(imgs))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "ds = VideoDataset(DATA_DIR, make_p_cut=True, make_b_cut=False)\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    ds,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=6,\n",
    "    prefetch_factor=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7944448 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Return number of parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(video_net)} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_to_psnr(mse):\n",
    "    return -10.0*np.log10(mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, epoch, dl, optimizer, train_type):\n",
    "    mse_criterion = torch.nn.MSELoss()\n",
    "    model.train()\n",
    "    \n",
    "    if train_type == 'memc':\n",
    "        mse_meter = meter.AverageValueMeter()\n",
    "    elif train_type == 'memc_bpp':\n",
    "        mse_meter = meter.AverageValueMeter()\n",
    "        bpp_meter = meter.AverageValueMeter()\n",
    "    elif train_type == 'recon':\n",
    "        mse_meter = meter.AverageValueMeter()\n",
    "    else:\n",
    "        mse_meter = meter.AverageValueMeter()\n",
    "        bpp_meter = meter.AverageValueMeter()\n",
    "        bpp_mv_y_meter = meter.AverageValueMeter()\n",
    "        bpp_mv_z_meter = meter.AverageValueMeter()\n",
    "        bpp_y_meter = meter.AverageValueMeter()\n",
    "        bpp_z_meter = meter.AverageValueMeter()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pbar = tqdm.tqdm(dl)\n",
    "    for i, x in enumerate(pbar):\n",
    "        x = x.to(DEVICE)\n",
    "        ref = x[:,0]\n",
    "        im = x[:,1]\n",
    "        preds_dict = model(ref, im, compress_type='train_compress', train_type=train_type)\n",
    "        if train_type == 'memc':\n",
    "            mse = mse_criterion(preds_dict['pred'], im)\n",
    "            mse.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse_meter.add(mse.item())\n",
    "            wandb_export = {\n",
    "                'train/psnr': mse_to_psnr(mse_meter.value()[0]),\n",
    "            }\n",
    "        elif train_type == 'memc_bpp':\n",
    "            mse = mse_criterion(preds_dict['pred'], im)\n",
    "            bpp = preds_dict['mv_z_bpp'] + preds_dict['mv_y_bpp']\n",
    "            loss = mse * LAMBDA + bpp\n",
    "            loss.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse_meter.add(mse.item())\n",
    "            bpp_meter.add(bpp.item())\n",
    "            wandb_export = {\n",
    "                'train/psnr': mse_to_psnr(mse_meter.value()[0]),\n",
    "                'train/bpp': bpp_meter.value()[0]\n",
    "            }\n",
    "        elif train_type == 'recon':\n",
    "            mse = mse_criterion(preds_dict['recon_image'], im)\n",
    "            mse.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse_meter.add(mse.item())\n",
    "            wandb_export = {\n",
    "                'train/psnr_recon': mse_to_psnr(mse_meter.value()[0]),\n",
    "            }\n",
    "        else:\n",
    "            mse = mse_criterion(preds_dict['recon_image'], im)\n",
    "            loss = mse * LAMBDA + preds_dict['bpp']\n",
    "            loss.backward()\n",
    "            \n",
    "            # metrics\n",
    "            mse_meter.add(mse.item())\n",
    "            bpp_meter.add(preds_dict['bpp'].item())\n",
    "            bpp_mv_y_meter.add(preds_dict['bpp_mv_y'].item())\n",
    "            bpp_mv_z_meter.add(preds_dict['bpp_mv_z'].item())\n",
    "            bpp_y_meter.add(preds_dict['bpp_y'].item())\n",
    "            bpp_z_meter.add(preds_dict['bpp_z'].item())\n",
    "            \n",
    "            wandb_export = {\n",
    "                'train/psnr_recon': mse_to_psnr(mse_meter.value()[0]),\n",
    "                'train/bpp': bpp_meter.value()[0],\n",
    "                'train/bpp_mv_y': bpp_mv_y_meter.value()[0],\n",
    "                'train/bpp_mv_z': bpp_mv_z_meter.value()[0],\n",
    "                'train/bpp_y': bpp_y_meter.value()[0],\n",
    "                'train/bpp_z': bpp_z_meter.value()[0],\n",
    "            }\n",
    "            \n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if i % 1 == 0:\n",
    "            wandb_export['train/epoch'] = epoch\n",
    "            wandb_export['train/train_type'] = train_type\n",
    "            wandb.log(wandb_export)\n",
    "        # save every \n",
    "        if i % 4000 == 3999:\n",
    "            print('Saving model')\n",
    "            torch.save(\n",
    "                {'model': model.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "                SAVE_FOLDER / f\"dcvc_epoch={epoch}_batch_{i}.pt\",\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_FOLDER = pathlib.Path(exp_name)\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layer(l):\n",
    "    for p in l.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_layer(l):\n",
    "    for p in l.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE LAMBDA True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (39.220927, 0.062847, 0.246654):  17%|█▋        | 3999/22926 [17:05<1:20:39,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 39.220927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (38.977758, 0.067147, 0.499104):  35%|███▍      | 8000/22926 [34:09<1:13:29,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 38.977758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (38.869666, 0.069992, 0.829218):  52%|█████▏    | 12000/22926 [51:13<53:27,  3.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 38.869666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (38.385357, 0.071914, 0.397238):  70%|██████▉   | 16000/22926 [1:08:17<34:26,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 38.385357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (38.225119, 0.072755, 0.295643):  87%|████████▋ | 20000/22926 [1:25:22<14:22,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 38.225119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (38.190542, 0.072985, 0.250607): 100%|██████████| 22926/22926 [1:37:51<00:00,  3.90it/s]\n",
      "Avg PSNR/Bitrate, Batch Loss: (38.053746, 0.074942, 0.344783):  17%|█▋        | 4000/22926 [17:05<1:34:23,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 38.053746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (38.022941, 0.075042, 0.402999):  35%|███▍      | 8000/22926 [34:09<1:14:34,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 38.022941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (38.062079, 0.074641, 0.346605):  52%|█████▏    | 12000/22926 [51:14<54:08,  3.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 38.062079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (38.041525, 0.074999, 0.210176):  70%|██████▉   | 16000/22926 [1:08:18<34:18,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 38.041525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.918479, 0.076044, 0.397714):  87%|████████▋ | 20000/22926 [1:25:21<14:37,  3.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.918479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.862637, 0.077274, 0.748301): 100%|██████████| 22926/22926 [1:37:51<00:00,  3.90it/s]\n",
      "Avg PSNR/Bitrate, Batch Loss: (37.281517, 0.08414, 0.235896):  17%|█▋        | 4000/22926 [17:04<1:35:03,  3.32it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.281517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.499517, 0.083779, 0.278157):  35%|███▍      | 8000/22926 [34:07<1:13:39,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.499517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.554326, 0.083896, 0.459263):  52%|█████▏    | 12000/22926 [51:12<53:50,  3.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.554326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.598734, 0.084145, 0.461153):  70%|██████▉   | 16000/22926 [1:08:16<33:58,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.598734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.617796, 0.084297, 0.554207):  87%|████████▋ | 20000/22926 [1:25:20<14:31,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.617796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.625801, 0.084552, 0.316915): 100%|██████████| 22926/22926 [1:37:48<00:00,  3.91it/s]\n",
      "Avg PSNR/Bitrate, Batch Loss: (37.643878, 0.088164, 0.456853):  17%|█▋        | 4000/22926 [17:05<1:34:50,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.643878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.590251, 0.088868, 0.440065):  35%|███▍      | 8000/22926 [34:09<1:13:48,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.590251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.596512, 0.089813, 0.543199):  52%|█████▏    | 12000/22926 [51:14<53:44,  3.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.596512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.538685, 0.090109, 0.324686):  70%|██████▉   | 16000/22926 [1:08:18<34:16,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.538685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.54284, 0.090848, 0.420329):  87%|████████▋ | 20000/22926 [1:25:22<14:26,  3.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.54284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.420717, 0.091282, 0.95865): 100%|██████████| 22926/22926 [1:37:52<00:00,  3.90it/s]  \n",
      "Avg PSNR/Bitrate, Batch Loss: (37.570614, 0.094544, 0.327195):  17%|█▋        | 4000/22926 [17:05<1:33:55,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with avg psnr 37.570614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg PSNR/Bitrate, Batch Loss: (37.523306, 0.094739, 0.34857):  19%|█▉        | 4451/22926 [19:00<1:18:54,  3.90it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE LAMBDA\u001b[39m\u001b[38;5;124m\"\u001b[39m, USE_LAMBDA)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     avg_loss, had_err, err_x \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSE_LAMBDA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m had_err:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBreaking out of train loop for debugging...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, epoch, dl, optimizer, criterion, use_lambda)\u001b[0m\n\u001b[1;32m     35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mse_loss\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 37\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     40\u001b[0m ls \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/DCVC/lib/python3.8/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DCVC/lib/python3.8/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    107\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/DCVC/lib/python3.8/site-packages/torch/optim/functional.py:86\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     83\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m     87\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "freeze_layer(video_net.opticFlow)\n",
    "\n",
    "train_type = 'memc'\n",
    "train_epoch(video_net, 1, dl, optimizer, train_type=train_type)\n",
    "torch.save(\n",
    "    {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "    SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    ")\n",
    "\n",
    "train_type = 'memc_bpp'\n",
    "for i in range(1, 4):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "\n",
    "# freeze mv layers\n",
    "mv_layers = [\n",
    "    video_net.bitEstimator_z_mv,\n",
    "    video_net.mvpriorEncoder,\n",
    "    video_net.mvpriorDecoder,\n",
    "    video_net.mvDecoder_part1,\n",
    "    video_net.mvDecoder_part2,\n",
    "    video_net.auto_regressive_mv,\n",
    "    video_net.entropy_parameters_mv,\n",
    "]\n",
    "print('Freezing MV layers')\n",
    "for l in mv_layers:\n",
    "    freeze_layer(l)\n",
    "\n",
    "train_type = 'recon'\n",
    "for i in range(4, 10):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "\n",
    "train_type = 'full'\n",
    "for i in range(10, 13):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "    \n",
    "# now unfreeze\n",
    "print('Unfreezing MV layers and optical flow for end-to-end training')\n",
    "for l in [video_net.opticFlow] + mv_layers:\n",
    "    unfreeze_layer(l)\n",
    "    \n",
    "for i in range(13, 19):\n",
    "    train_epoch(video_net, i, dl, optimizer, train_type=train_type)\n",
    "    torch.save(\n",
    "        {'model': video_net.state_dict(), 'optimizer': optimizer.state_dict()},\n",
    "        SAVE_FOLDER / f\"dcvc_{train_type}_epoch={i}.pt\",\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcvc",
   "language": "python",
   "name": "dcvc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
